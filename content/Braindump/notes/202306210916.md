---
title: Bayesian surprise
draft: false
---
# Bayesian surprise
How different the prior is from the posterior. How much updating happens in light of the new data. The Kullback-Leibler (KL) Divergence can be used as a representation of the difference between the distributions. 

$$D_{KL}[Q(x)||P(x)] = \mathbb{E}_{Q(x)}[\ln Q(x) - \ln P(x)]$$

---
### References
[[@parr2022b]]